{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a77ba11-884f-4efd-9b12-a2c6c6a51c9f",
   "metadata": {},
   "source": [
    "<p style=\"font-size:28px;font-weight:bold;\">Module 1 - Data Input</p>\n",
    "In order to simplify the handling of data further on, this module is responsible for reading in all audio data, performing the feature extraction and labeling, and saving the result to Numpy files. </br>\n",
    "</br>\n",
    "Data will be read in from all wav files in the dataset directory, with the labels being determined by the path name. Do not place any unrelated wav files, as they will cause issues.</br>\n",
    "For example, files in : '.\\Dataset\\Pump 1\\run\\' will be labeled as Pump 1 Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ca8e92-5141-43ce-889f-58f6bd5c040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import torchaudio\n",
    "from torchaudio import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef083fde-e516-4a19-99c5-dd3d5ad5fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recordings: 918\n"
     ]
    }
   ],
   "source": [
    "## For labels, pumps 1-4 = labels 0, 3, 6, and 9\n",
    "## For run state, add: start = 0, run = 1, stop = 2\n",
    "\n",
    "samples = 0;\n",
    "for root,dirs,files in os.walk(\".\\\\Dataset\", topdown=True):\n",
    "    for name in files:\n",
    "        if(\".wav\" in name):\n",
    "            samples += 1;\n",
    "paths = [];\n",
    "labels= np.zeros(samples);\n",
    "print(f\"Number of recordings: {labels.shape[0]}\");\n",
    "i=0;\n",
    "for root,dirs,files in os.walk(\".\\\\Dataset\", topdown=True):\n",
    "    for name in files:\n",
    "        if(\"Pump 1\" in root):\n",
    "            paths.append(os.path.join(root,name));\n",
    "            labels[i] = 0;\n",
    "            if(\"run\" in root):\n",
    "                labels[i] += 1;\n",
    "            elif(\"start\" in root):\n",
    "                labels[i] += 0;\n",
    "            elif(\"stop\" in root):\n",
    "                labels[i] += 2;\n",
    "        elif(\"Pump 2\" in root):\n",
    "            paths.append(os.path.join(root,name));\n",
    "            labels[i] = 3;\n",
    "            if(\"run\" in root):\n",
    "                labels[i] += 1;\n",
    "            elif(\"start\" in root):\n",
    "                labels[i] += 0;\n",
    "            elif(\"stop\" in root):\n",
    "                labels[i] += 2;\n",
    "        elif(\"Pump 3\" in root):\n",
    "            paths.append(os.path.join(root,name));\n",
    "            labels[i] = 6;\n",
    "            if(\"run\" in root):\n",
    "                labels[i] += 1;\n",
    "            elif(\"start\" in root):\n",
    "                labels[i] += 0;\n",
    "            elif(\"stop\" in root):\n",
    "                labels[i] += 2;\n",
    "        elif(\"Pump 4\" in root):\n",
    "            paths.append(os.path.join(root,name));\n",
    "            labels[i] = 9;\n",
    "            if(\"run\" in root):\n",
    "                labels[i] += 1;\n",
    "            elif(\"start\" in root):\n",
    "                labels[i] += 0;\n",
    "            elif(\"stop\" in root):\n",
    "                labels[i] += 2;\n",
    "        else:\n",
    "            print(\"Bad File \" + str(i));\n",
    "            print(os.path.join(root,name));\n",
    "            i += -1;\n",
    "        i += 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be80b5-c332-4fea-a40c-28b051f60cb1",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px;font-weight:bold;\">Feature Extractions</p>\n",
    "The following lines perform feature extraction on the files searched above. This includes:</br>\n",
    "<ul>\n",
    "    <li>Mel-Frequency Cepstrum Coefficients</li>\n",
    "    <li>Linear-Frequency Cepstrum Coefficients</li>\n",
    "    <li>Linear Prediction Coefficients</li>\n",
    "</ul>\n",
    "The MFCC and LPC processing is performed by the librosa library, while the LFCC processing is performed by TorchAudio, an extension of PyTorch</br>\n",
    "The number of samples read from each file is capped by the value determined to be the length of the shortest recording. This is so dimensionality remains constant across all recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475c9c4c-d1cb-41d1-bc5f-eef440acd23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\374177\\AppData\\Local\\Temp\\ipykernel_39608\\787428995.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  samples_LFCC[count] = transform(y[0][:shortestSamples])\n"
     ]
    }
   ],
   "source": [
    "shortestSamples = 46298*2;                    # This value is the length (in samples) of the shortest recording in the set.\n",
    "samples_LPC = np.zeros((len(paths),14));      # LPC returns a 1x(1+order) array for each recording\n",
    "samples_LFCC = np.zeros((len(paths),13,463)); # LFCC Returns an (n_lfcc)x(463) array for this length of recording\n",
    "samples_MFCC = np.zeros((len(paths),13,181)); # MFCC Returns an (n_mfcc)x(181) array for this length of recording\n",
    "\n",
    "count = 0;\n",
    "for path in paths:\n",
    "    y,sr = librosa.load(path,sr=None);\n",
    "    samples_LPC[count] = librosa.lpc(y[:shortestSamples],order=13)\n",
    "    samples_MFCC[count] = librosa.feature.mfcc(y=y[:shortestSamples], sr=sr, n_mfcc=13);\n",
    "    \n",
    "    y,sr = torchaudio.load(path);\n",
    "    transform = transforms.LFCC(sample_rate = sr, n_lfcc=13);\n",
    "    samples_LFCC[count] = transform(y[0][:shortestSamples])\n",
    "    count += 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a0564-435c-4fcd-bda9-fbbd18ad265a",
   "metadata": {},
   "source": [
    "The outputs from these feature extractions are 3-Dimensional for all but the Linear Prediction Coefficients. We can save these directly for use with the Neural Networks, but to improve handling with traditional learning methods, we will also transform the data to a 2-Dimensional set and save that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2153f4-39d5-4ae7-8205-e8997c71ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"3D_LFCC.npy\",samples_LFCC);\n",
    "np.save(\"3D_MFCC.npy\",samples_MFCC);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303227ea-0c4c-439b-9d1f-cbede580d135",
   "metadata": {},
   "source": [
    "In order to reshape the 3D data to 2D, we use the numpy reshape call. </br>This function takes the dimension of the first dimension, which we want to remain constant, and by passing -1 to the second argument, we reduce dimensionality by one. </br>This transforms our data from a <b>[recordings]x[mfccs]x[time]</b> array to a <b>[recordings]x[mfccs*time]</b> array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0701651-d4eb-48e0-a462-d21cb1d180f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_LFCC2D = samples_LFCC.reshape(samples_LFCC.shape[0],-1);\n",
    "samples_MFCC2D = samples_MFCC.reshape(samples_MFCC.shape[0],-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086fcedb-7c92-4f26-bfb8-84419a97d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3D MFCC data is (918, 13, 181) in dimension\n",
      "The 3D LFCC data is (918, 13, 463) in dimension\n",
      "The 2D MFCC data is (918, 2353) in dimension\n",
      "The 2D LFCC data is (918, 6019) in dimension\n"
     ]
    }
   ],
   "source": [
    "print(f\"The 3D MFCC data is {samples_MFCC.shape} in dimension\");\n",
    "print(f\"The 3D LFCC data is {samples_LFCC.shape} in dimension\");\n",
    "print(f\"The 2D MFCC data is {samples_MFCC2D.shape} in dimension\");\n",
    "print(f\"The 2D LFCC data is {samples_LFCC2D.shape} in dimension\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c519c9f8-1444-459c-bc14-26627c00dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"2D_LPC.npy\",samples_LPC);\n",
    "np.save(\"2D_MFCC.npy\", samples_MFCC2D);\n",
    "np.save(\"2D_LFCC.npy\", samples_LFCC2D);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3e26c-bb5d-4b09-bdee-630fa47010fc",
   "metadata": {},
   "source": [
    "Finally, we need to save the labels to a Numpy file as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7fc57e-1f08-412a-b7ed-97c3495876f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Labels.npy\", labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06dfd95-ddb0-46f6-90ea-7a14a10c9a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
