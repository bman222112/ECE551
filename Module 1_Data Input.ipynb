{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bman222112/ECE551/blob/main/Module%201_Data%20Input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a77ba11-884f-4efd-9b12-a2c6c6a51c9f",
      "metadata": {
        "id": "4a77ba11-884f-4efd-9b12-a2c6c6a51c9f"
      },
      "source": [
        "<p style=\"font-size:28px;font-weight:bold;\">Module 1 - Data Input</p>\n",
        "In order to simplify the handling of data further on, this module is responsible for reading in all audio data, performing the feature extraction and labeling, and saving the result to Numpy files. </br>\n",
        "</br>\n",
        "Data will be read in from all wav files in the dataset directory, with the labels being determined by the path name. Do not place any unrelated wav files, as they will cause issues.</br>\n",
        "For example, files in : '.\\Dataset\\Pump 1\\run\\' will be labeled as Pump 1 Run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs"
      ],
      "metadata": {
        "id": "_c2W7rutgIBP",
        "outputId": "6573c422-24d0-457e-e0d6-1b2c3a7fecbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_c2W7rutgIBP",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/jammy.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Detected apt version as 2.4.14\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
            "done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following packages will be upgraded:\n",
            "  git-lfs\n",
            "1 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 8,917 kB of archives.\n",
            "After this operation, 8,712 kB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu jammy/main amd64 git-lfs amd64 3.7.0 [8,917 kB]\n",
            "Fetched 8,917 kB in 1s (9,218 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 126288 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_3.7.0_amd64.deb ...\n",
            "Unpacking git-lfs (3.7.0) over (3.0.2-1ubuntu0.3) ...\n",
            "Setting up git-lfs (3.7.0) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://www.github.com/bman222112/ECE551.git\n",
        "%cd /content/ECE551"
      ],
      "metadata": {
        "id": "fydEebxDgOkY",
        "outputId": "52760dd4-a7e1-448d-ba61-0bec79930af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fydEebxDgOkY",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'ECE551'...\n",
            "warning: redirecting to https://github.com/bman222112/ECE551.git/\n",
            "remote: Enumerating objects: 1015, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 1015 (delta 1), reused 0 (delta 0), pack-reused 1009 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1015/1015), 747.47 MiB | 16.57 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Updating files: 100% (953/953), done.\n",
            "Filtering content: 100% (2/2), 252.93 MiB | 27.77 MiB/s, done.\n",
            "/content/ECE551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "60ca8e92-5141-43ce-889f-58f6bd5c040d",
      "metadata": {
        "id": "60ca8e92-5141-43ce-889f-58f6bd5c040d"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import torchaudio\n",
        "from torchaudio import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ef083fde-e516-4a19-99c5-dd3d5ad5fc41",
      "metadata": {
        "id": "ef083fde-e516-4a19-99c5-dd3d5ad5fc41",
        "outputId": "370780d2-9888-4a03-b861-b3352dc0463e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of recordings: 918\n"
          ]
        }
      ],
      "source": [
        "## For labels, pumps 1-4 = labels 0, 3, 6, and 9\n",
        "## For run state, add: start = 0, run = 1, stop = 2\n",
        "\n",
        "samples = 0;\n",
        "for root,dirs,files in os.walk(\"./Dataset\", topdown=True):\n",
        "    for name in files:\n",
        "        if(\".wav\" in name):\n",
        "            samples += 1;\n",
        "paths = [];\n",
        "labels= np.zeros(samples);\n",
        "print(f\"Number of recordings: {labels.shape[0]}\");\n",
        "i=0;\n",
        "for root,dirs,files in os.walk(\"./Dataset\", topdown=True):\n",
        "    for name in files:\n",
        "        if(\"Pump 1\" in root):\n",
        "            paths.append(os.path.join(root,name));\n",
        "            labels[i] = 0;\n",
        "            if(\"run\" in root):\n",
        "                labels[i] += 1;\n",
        "            elif(\"start\" in root):\n",
        "                labels[i] += 0;\n",
        "            elif(\"stop\" in root):\n",
        "                labels[i] += 2;\n",
        "        elif(\"Pump 2\" in root):\n",
        "            paths.append(os.path.join(root,name));\n",
        "            labels[i] = 3;\n",
        "            if(\"run\" in root):\n",
        "                labels[i] += 1;\n",
        "            elif(\"start\" in root):\n",
        "                labels[i] += 0;\n",
        "            elif(\"stop\" in root):\n",
        "                labels[i] += 2;\n",
        "        elif(\"Pump 3\" in root):\n",
        "            paths.append(os.path.join(root,name));\n",
        "            labels[i] = 6;\n",
        "            if(\"run\" in root):\n",
        "                labels[i] += 1;\n",
        "            elif(\"start\" in root):\n",
        "                labels[i] += 0;\n",
        "            elif(\"stop\" in root):\n",
        "                labels[i] += 2;\n",
        "        elif(\"Pump 4\" in root):\n",
        "            paths.append(os.path.join(root,name));\n",
        "            labels[i] = 9;\n",
        "            if(\"run\" in root):\n",
        "                labels[i] += 1;\n",
        "            elif(\"start\" in root):\n",
        "                labels[i] += 0;\n",
        "            elif(\"stop\" in root):\n",
        "                labels[i] += 2;\n",
        "        else:\n",
        "            print(\"Bad File \" + str(i));\n",
        "            print(os.path.join(root,name));\n",
        "            i += -1;\n",
        "        i += 1;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3be80b5-c332-4fea-a40c-28b051f60cb1",
      "metadata": {
        "id": "d3be80b5-c332-4fea-a40c-28b051f60cb1"
      },
      "source": [
        "<p style=\"font-size:20px;font-weight:bold;\">Feature Extractions</p>\n",
        "The following lines perform feature extraction on the files searched above. This includes:</br>\n",
        "<ul>\n",
        "    <li>Mel-Frequency Cepstrum Coefficients</li>\n",
        "    <li>Linear-Frequency Cepstrum Coefficients</li>\n",
        "    <li>Linear Prediction Coefficients</li>\n",
        "</ul>\n",
        "The MFCC and LPC processing is performed by the librosa library, while the LFCC processing is performed by TorchAudio, an extension of PyTorch</br>\n",
        "The number of samples read from each file is capped by the value determined to be the length of the shortest recording. This is so dimensionality remains constant across all recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "475c9c4c-d1cb-41d1-bc5f-eef440acd23a",
      "metadata": {
        "id": "475c9c4c-d1cb-41d1-bc5f-eef440acd23a",
        "outputId": "98090289-3edb-4d0c-dcaf-04e2866a8742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-787428995.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
            "  samples_LFCC[count] = transform(y[0][:shortestSamples])\n"
          ]
        }
      ],
      "source": [
        "shortestSamples = 46298*2;                    # This value is the length (in samples) of the shortest recording in the set.\n",
        "samples_LPC = np.zeros((len(paths),14));      # LPC returns a 1x(1+order) array for each recording\n",
        "samples_LFCC = np.zeros((len(paths),13,463)); # LFCC Returns an (n_lfcc)x(463) array for this length of recording\n",
        "samples_MFCC = np.zeros((len(paths),13,181)); # MFCC Returns an (n_mfcc)x(181) array for this length of recording\n",
        "\n",
        "count = 0;\n",
        "for path in paths:\n",
        "    y,sr = librosa.load(path,sr=None);\n",
        "    samples_LPC[count] = librosa.lpc(y[:shortestSamples],order=13)\n",
        "    samples_MFCC[count] = librosa.feature.mfcc(y=y[:shortestSamples], sr=sr, n_mfcc=13);\n",
        "\n",
        "    y,sr = torchaudio.load(path);\n",
        "    transform = transforms.LFCC(sample_rate = sr, n_lfcc=13);\n",
        "    samples_LFCC[count] = transform(y[0][:shortestSamples])\n",
        "    count += 1;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053a0564-435c-4fcd-bda9-fbbd18ad265a",
      "metadata": {
        "id": "053a0564-435c-4fcd-bda9-fbbd18ad265a"
      },
      "source": [
        "The outputs from these feature extractions are 3-Dimensional for all but the Linear Prediction Coefficients. We can save these directly for use with the Neural Networks, but to improve handling with traditional learning methods, we will also transform the data to a 2-Dimensional set and save that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4f2153f4-39d5-4ae7-8205-e8997c71ef04",
      "metadata": {
        "id": "4f2153f4-39d5-4ae7-8205-e8997c71ef04"
      },
      "outputs": [],
      "source": [
        "np.save(\"3D_LFCC.npy\",samples_LFCC);\n",
        "np.save(\"3D_MFCC.npy\",samples_MFCC);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303227ea-0c4c-439b-9d1f-cbede580d135",
      "metadata": {
        "id": "303227ea-0c4c-439b-9d1f-cbede580d135"
      },
      "source": [
        "In order to reshape the 3D data to 2D, we use the numpy reshape call. </br>This function takes the dimension of the first dimension, which we want to remain constant, and by passing -1 to the second argument, we reduce dimensionality by one. </br>This transforms our data from a <b>[recordings]x[mfccs]x[time]</b> array to a <b>[recordings]x[mfccs*time]</b> array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b0701651-d4eb-48e0-a462-d21cb1d180f6",
      "metadata": {
        "id": "b0701651-d4eb-48e0-a462-d21cb1d180f6"
      },
      "outputs": [],
      "source": [
        "samples_LFCC2D = samples_LFCC.reshape(samples_LFCC.shape[0],-1);\n",
        "samples_MFCC2D = samples_MFCC.reshape(samples_MFCC.shape[0],-1);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "086fcedb-7c92-4f26-bfb8-84419a97d5e5",
      "metadata": {
        "id": "086fcedb-7c92-4f26-bfb8-84419a97d5e5",
        "outputId": "efc1127e-6762-4cd8-cbf1-d61b688220c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 3D MFCC data is (918, 13, 181) in dimension\n",
            "The 3D LFCC data is (918, 13, 463) in dimension\n",
            "The 2D MFCC data is (918, 2353) in dimension\n",
            "The 2D LFCC data is (918, 6019) in dimension\n"
          ]
        }
      ],
      "source": [
        "print(f\"The 3D MFCC data is {samples_MFCC.shape} in dimension\");\n",
        "print(f\"The 3D LFCC data is {samples_LFCC.shape} in dimension\");\n",
        "print(f\"The 2D MFCC data is {samples_MFCC2D.shape} in dimension\");\n",
        "print(f\"The 2D LFCC data is {samples_LFCC2D.shape} in dimension\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c519c9f8-1444-459c-bc14-26627c00dbb4",
      "metadata": {
        "id": "c519c9f8-1444-459c-bc14-26627c00dbb4"
      },
      "outputs": [],
      "source": [
        "np.save(\"2D_LPC.npy\",samples_LPC);\n",
        "np.save(\"2D_MFCC.npy\", samples_MFCC2D);\n",
        "np.save(\"2D_LFCC.npy\", samples_LFCC2D);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba3e26c-bb5d-4b09-bdee-630fa47010fc",
      "metadata": {
        "id": "0ba3e26c-bb5d-4b09-bdee-630fa47010fc"
      },
      "source": [
        "Finally, we need to save the labels to a Numpy file as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fc7fc57e-1f08-412a-b7ed-97c3495876f3",
      "metadata": {
        "id": "fc7fc57e-1f08-412a-b7ed-97c3495876f3"
      },
      "outputs": [],
      "source": [
        "np.save(\"Labels.npy\", labels);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a06dfd95-ddb0-46f6-90ea-7a14a10c9a8c",
      "metadata": {
        "id": "a06dfd95-ddb0-46f6-90ea-7a14a10c9a8c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}